{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d423dd-4078-4687-abb8-5e68b00931bc",
   "metadata": {},
   "source": [
    "# URL Similarity Search System using ChromaDB\n",
    "-----------------------------------------\n",
    "\n",
    "In the digital age, dealing with broken or incorrect URLs is a common challenge that can lead to poor user experience and lost traffic. This project implements a smart URL matching system that helps users find the correct webpage even when they encounter typos or slightly incorrect URLs. By converting URLs into numerical vectors and using ChromaDB for similarity search, the system can quickly identify and suggest the most similar valid URLs from a website's sitemap. This approach is particularly useful for large websites where manually redirecting or finding correct URLs would be time-consuming. The system processes XML sitemaps to build its knowledge base, and when given a potentially incorrect URL, it returns the closest matches based on semantic similarity rather than just character matching, making it more effective at understanding user intent.\n",
    "\n",
    "### Why this process is important\n",
    "\n",
    "**1.- User Experience & Error Recovery**\n",
    "\n",
    "*   Users often encounter broken links or mistyped URLs\n",
    "*   Instead of showing a generic 404 error, we can guide users to the content they likely meant to access\n",
    "*   Real-world example: A user types \"motortrend.com/news/ford-mustang-review-2024\" but the actual URL is \"motortrend.com/news/ford-mustang-2024-review\" - our system would redirect them to the correct page\n",
    "*   This reduces user frustration and maintains engagement on the site\n",
    "\n",
    "**2.- Content Migration & Legacy Support**\n",
    "\n",
    "*   Websites frequently reorganize their URL structure or migrate content\n",
    "*   Old bookmarks, external links, and search engine results may still point to previous URL patterns\n",
    "*   Our system can help maintain continuity by:\n",
    "    *   Finding the new location of moved content\n",
    "    *   Handling various URL formats that might have been used over time\n",
    "    *   Preserving SEO value from old links by providing relevant alternatives\n",
    "\n",
    "**3.-Analytics & Content Management**\n",
    "\n",
    "*   Understanding URL patterns and similarities helps content teams:\n",
    "    *   Identify duplicate or similar content that might need consolidation\n",
    "    *   Analyze how content is organized and accessed\n",
    "    *   Track content evolution over time through URL changes\n",
    "    *   Make data-driven decisions about content structure\n",
    "*   Example: If many users are searching for \"reviews\" in different URL formats, it might indicate a need to standardize the URL structure for review content\n",
    "\n",
    "\n",
    "This module implements a URL similarity search system that can:\n",
    "1. Process XML sitemaps to extract URLs\n",
    "2. Convert URLs into vector representations\n",
    "3. Store these vectors in ChromaDB for efficient similarity search\n",
    "4. Find similar URLs when given a potentially incorrect URL\n",
    "\n",
    "The system is particularly useful for:\n",
    "- Finding correct URLs when users mistype or remember URLs incorrectly\n",
    "- Redirecting users to the closest matching content\n",
    "- Analyzing URL patterns in a website\n",
    "\n",
    "Requirements:\n",
    "- chromadb\n",
    "- numpy\n",
    "- tqdm\n",
    "- xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c205db71-3073-4813-a38b-e4f21dcdcec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.115.8)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (3.11.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (32.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from build>=1.0.3->chromadb) (8.5.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from importlib-resources->chromadb) (3.21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Requirement already satisfied: protobuf in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\getapia\\appdata\\local\\anaconda3\\envs\\tensor2\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # URL Similarity Matcher using ChromaDB\n",
    "# This notebook processes XML sitemaps and creates a similarity search system for URLs.\n",
    "# \n",
    "# ## Setup\n",
    "# First, let's install the required packages:\n",
    "\n",
    "# %%\n",
    "# Install required packages\n",
    "!pip install chromadb tqdm numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab99f737-bcc7-481d-ba1b-0d82b1395b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d732ee2-5c78-47f0-b7fb-8c1aae639a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLMatcher:\n",
    "    \"\"\"\n",
    "    A class that manages URL similarity matching using vector embeddings and ChromaDB.\n",
    "    \n",
    "    This class provides functionality to:\n",
    "    - Process XML sitemaps and extract URLs\n",
    "    - Convert URLs into vector representations\n",
    "    - Store URL vectors in ChromaDB\n",
    "    - Find similar URLs using vector similarity search\n",
    "    \n",
    "    Attributes:\n",
    "        sitemaps_folder (str): Path to folder containing XML sitemaps\n",
    "        client (chromadb.Client): ChromaDB client instance\n",
    "        collection (chromadb.Collection): ChromaDB collection for storing URLs\n",
    "        debugging (bool): Flag to control debug output\n",
    "\n",
    "    This class is using cosine similarity between vectors\n",
    "    \"\"\"    \n",
    "    def __init__(self, collection_name: str = \"url_collection1\", sitemaps_folder=\"C:\\\\Users\\\\getapia\\\\ML_RAG_Project\\\\sitemap\"):\n",
    "        \"\"\"\n",
    "        Initialize the URL matcher with a collection name and sitemaps folder.\n",
    "        \n",
    "        Args:\n",
    "            collection_name (str): Name of the ChromaDB collection to use\n",
    "            sitemaps_folder (str): Path to folder containing XML sitemaps\n",
    "        \"\"\"        \n",
    "        self.sitemaps_folder = sitemaps_folder\n",
    "        self.client = chromadb.Client()\n",
    "        self.debugging = False\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=embedding_functions.DefaultEmbeddingFunction(),\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "            if self.debugging:\n",
    "                print(f\"Retrieved existing collection: {collection_name}\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=embedding_functions.DefaultEmbeddingFunction(),\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "            if self.debugging:\n",
    "                print(f\"Created new collection: {collection_name}\")\n",
    "            \n",
    "    def normalize_path(self, url_or_path):\n",
    "        \"\"\"\n",
    "        Normalize a URL or path to a consistent format.\n",
    "        \n",
    "        Args:\n",
    "            url_or_path (str): Full URL or path to normalize\n",
    "            \n",
    "        Returns:\n",
    "            str: Normalized path starting with '/'\n",
    "            \n",
    "        Example:\n",
    "            'https://example.com/path/to/page/' -> '/path/to/page'\n",
    "        \"\"\"        \n",
    "        if not url_or_path:\n",
    "            return \"\"\n",
    "            \n",
    "        if url_or_path.startswith('http'):\n",
    "            parsed = urlparse(url_or_path)\n",
    "            path = parsed.path\n",
    "        else:\n",
    "            path = url_or_path\n",
    "            \n",
    "        path = path.strip('/')\n",
    "        \n",
    "        # Remove empty segments\n",
    "        path_parts = [p for p in path.split('/') if p]\n",
    "        \n",
    "        # Rejoin with single slashes\n",
    "        return '/' + '/'.join(path_parts)\n",
    "\n",
    "    def url_to_vector(self, url_or_path):\n",
    "        \"\"\"\n",
    "        Convert a URL or path to a vector representation using semantic understanding.\n",
    "        \n",
    "        This method:\n",
    "        1. Normalizes the path\n",
    "        2. Extracts the content part (usually the last segment)\n",
    "        3. Splits into words\n",
    "        4. Creates trigrams for each word\n",
    "        5. Weights features based on position\n",
    "        6. Normalizes the final vector\n",
    "        \n",
    "        Args:\n",
    "            url_or_path (str): URL or path to vectorize\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Normalized vector representation of the URL\n",
    "        \"\"\"\n",
    "        # Normalize the path first\n",
    "        path = self.normalize_path(url_or_path)\n",
    "        \n",
    "        # Split into segments\n",
    "        path_parts = path.strip('/').split('/')\n",
    "        \n",
    "        # Get the relevant parts (usually the last part contains the actual content)\n",
    "        if len(path_parts) > 1:\n",
    "            content_part = path_parts[-1]  # Get the last segment\n",
    "        else:\n",
    "            content_part = path_parts[0]\n",
    "            \n",
    "        # Split by hyphens to get individual words\n",
    "        words = content_part.split('-')\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Process each word\n",
    "        for word in words:\n",
    "            # Remove common suffixes like numbers and years\n",
    "            word = ''.join([c for c in word if not c.isdigit()])\n",
    "            \n",
    "            # Get trigrams of the word to capture word structure\n",
    "            trigrams = [word[i:i+3] for i in range(len(word)-2)]\n",
    "            \n",
    "            # Convert trigrams to numbers\n",
    "            for trigram in trigrams:\n",
    "                # Create a hash of the trigram\n",
    "                trigram_value = sum(ord(c) * (i+1) for i, c in enumerate(trigram))\n",
    "                features.append(trigram_value)\n",
    "        \n",
    "        # Weight the features based on word position\n",
    "        # Words at the start and end are usually more important\n",
    "        weighted_features = []\n",
    "        for i, value in enumerate(features):\n",
    "            position_weight = 1.0\n",
    "            if i < len(features) // 3:  # First third\n",
    "                position_weight = 1.5\n",
    "            elif i > (2 * len(features)) // 3:  # Last third\n",
    "                position_weight = 1.3\n",
    "            weighted_features.append(value * position_weight)\n",
    "        \n",
    "        # Ensure fixed length\n",
    "        target_length = 150\n",
    "        if len(weighted_features) < target_length:\n",
    "            weighted_features.extend([0] * (target_length - len(weighted_features)))\n",
    "        else:\n",
    "            weighted_features = weighted_features[:target_length]\n",
    "        \n",
    "        # Normalize vector\n",
    "        features = np.array(weighted_features, dtype=np.float32)\n",
    "        norm = np.linalg.norm(features)\n",
    "        if norm > 0:\n",
    "            features = features / norm\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def process_sitemap(self, file_path):\n",
    "        \"\"\"\n",
    "        Process a single sitemap XML file to extract URLs and last modification dates.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the sitemap XML file\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (list of URLs, list of last modification dates)\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            \n",
    "            urls = []\n",
    "            lastmods = []\n",
    "            counter = 0\n",
    "            \n",
    "            for url in root.findall('.//ns:url', ns):\n",
    "                loc = url.find('ns:loc', ns)\n",
    "                lastmod = url.find('ns:lastmod', ns)\n",
    "                \n",
    "                if loc is not None:\n",
    "                    full_url = loc.text\n",
    "                    normalized_path = self.normalize_path(full_url)\n",
    "                    \n",
    "                    if normalized_path:\n",
    "                        urls.append(normalized_path)\n",
    "                        lastmods.append(lastmod.text if lastmod is not None else None)\n",
    "                        counter += 1\n",
    "            \n",
    "            if self.debugging:\n",
    "                print(f'File-> {file_path} with {counter} paths processed')\n",
    "            return urls, lastmods\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "            return [], []\n",
    "\n",
    "    def process_all_sitemaps(self):\n",
    "        \"\"\"\n",
    "        Process all XML sitemap files in the specified folder.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (list of all URLs, list of all last modification dates)\n",
    "        \"\"\"\n",
    "        all_urls = []\n",
    "        all_lastmods = []\n",
    "        \n",
    "        xml_files = [f for f in os.listdir(self.sitemaps_folder) \n",
    "                    if f.endswith('.xml')]\n",
    "        \n",
    "        print(f\"Processing {len(xml_files)} sitemap files...\")\n",
    "        for xml_file in tqdm(xml_files):\n",
    "            file_path = os.path.join(self.sitemaps_folder, xml_file)\n",
    "            urls, lastmods = self.process_sitemap(file_path)\n",
    "            all_urls.extend(urls)\n",
    "            all_lastmods.extend(lastmods)\n",
    "        \n",
    "        return all_urls, all_lastmods\n",
    "\n",
    "    def store_urls(self, urls, lastmods):\n",
    "        \"\"\"\n",
    "        Store URLs and their vectors in ChromaDB.\n",
    "        \n",
    "        Args:\n",
    "            urls (list): List of URLs to store\n",
    "            lastmods (list): List of last modification dates\n",
    "        \"\"\"        \n",
    "        if self.debugging:\n",
    "            print(\"Converting URLs to vectors and storing in ChromaDB...\")\n",
    "        \n",
    "        batch_size = 1000\n",
    "        for i in tqdm(range(0, len(urls), batch_size)):\n",
    "            batch_urls = urls[i:i + batch_size]\n",
    "            batch_lastmods = lastmods[i:i + batch_size]\n",
    "            \n",
    "            vectors = [self.url_to_vector(url).tolist() for url in batch_urls]\n",
    "            metadatas = [{\"lastmod\": lm} for lm in batch_lastmods]\n",
    "            \n",
    "            self.collection.add(\n",
    "                embeddings=vectors,\n",
    "                documents=batch_urls,\n",
    "                metadatas=metadatas,\n",
    "                ids=[f\"url_{j}\" for j in range(i, i + len(batch_urls))]\n",
    "            )\n",
    "\n",
    "    def process_and_store(self):\n",
    "        \"\"\"Process all sitemaps and store URLs in ChromaDB.\"\"\"\n",
    "        urls, lastmods = self.process_all_sitemaps()\n",
    "        if self.debugging:\n",
    "            print(f\"Found {len(urls)} URLs\")\n",
    "        self.store_urls(urls, lastmods)\n",
    "        if self.debugging:\n",
    "            print(\"URLs stored successfully in ChromaDB\")\n",
    "\n",
    "    def find_similar_url_by_path(self, query_url, n_results=2):\n",
    "        \"\"\"\n",
    "        Find similar URLs to the query URL.\n",
    "        \n",
    "        Args:\n",
    "            query_url (str): URL to find matches for\n",
    "            n_results (int): Number of similar URLs to return\n",
    "            \n",
    "        Returns:\n",
    "            dict: ChromaDB query results containing similar URLs and their distances\n",
    "            \n",
    "        Example:\n",
    "            matcher.find_similar_url_by_path(\"https://example.com/incorrect-path\")\n",
    "        \"\"\"\n",
    "        # Normalize the query path\n",
    "        query_path = self.normalize_path(query_url)\n",
    "        \n",
    "        # Convert to vector\n",
    "        query_vector = self.url_to_vector(query_path)\n",
    "        \n",
    "        # Search in ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_vector.tolist()],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        # Check if exact path exists\n",
    "        exists = query_path in results['documents'][0]\n",
    "        if exists:\n",
    "            print(\"URL Exists in database\")\n",
    "        else:\n",
    "            print(\"URL doesn't exist in database, here closer results\\n\")\n",
    "            for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "                similarity = 1 - distance  # Convert distance to similarity score\n",
    "                print(f\"{i+1}. Path: https://www.motortrend.com{doc}/\")\n",
    "                print(f\"   Similarity: {similarity:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about the URL collection.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of URLs in the collection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            count = self.collection.count()\n",
    "            \n",
    "            sample = self.collection.get(\n",
    "                limit=5,\n",
    "                include=['documents', 'metadatas']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nCollection Statistics:\")\n",
    "            print(f\"Total number of stored URLs: {count}\")\n",
    "            \n",
    "            if count > 0:\n",
    "                print(\"\\nSample of stored URLs:\")\n",
    "                for i, doc in enumerate(sample['documents']):\n",
    "                    print(f\"{i+1}. {doc}\")\n",
    "                    print(f\"   Last modified: {sample['metadatas'][i]['lastmod']}\")\n",
    "            \n",
    "            return count\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting collection stats: {e}\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a42e7f4-ac49-42d4-a37b-c437413acda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the matcher\n",
    "matcher = URLMatcher(collection_name=\"url_collection1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c936476-da62-435a-896a-66deaa143675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Checking Collection Stats Before Processing ===\n",
      "\n",
      "Collection Statistics:\n",
      "Total number of stored URLs: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify the correct initialization/creation of the DB\n",
    "print(\"\\n=== Checking Collection Stats Before Processing ===\")\n",
    "count = matcher.get_collection_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0209d13-f73b-4ae8-b156-943c06c6a133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing and Storing URLs ===\n",
      "Processing 26 sitemap files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 38.12it/s]\n",
      "100%|██████████| 54/54 [00:21<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process and store URLs\n",
    "if count == 0:\n",
    "    print(\"\\n=== Processing and Storing URLs ===\")\n",
    "    matcher.process_and_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f59fa9-4d39-4f66-a41b-9f910f93756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Checking Collection Stats After Processing ===\n",
      "\n",
      "Collection Statistics:\n",
      "Total number of stored URLs: 53277\n",
      "\n",
      "Sample of stored URLs:\n",
      "1. /news/mustang-1964\n",
      "   Last modified: 2000-11-01T07:00:00.000Z\n",
      "2. /news/porsche-911-turbo\n",
      "   Last modified: 2000-09-02T06:42:00.000Z\n",
      "3. /news/inside-cadillacs-project-blackfin\n",
      "   Last modified: 2000-07-01T07:31:00.000Z\n",
      "4. /news/0004-turp-abiogenic-petroleum-theory\n",
      "   Last modified: 2000-04-01T05:52:00.000Z\n",
      "5. /news/82799-ford-focus-rally-car\n",
      "   Last modified: 2000-07-01T08:00:00.000Z\n"
     ]
    }
   ],
   "source": [
    "# Verify we load the URLs in the DB\n",
    "print(\"\\n=== Checking Collection Stats After Processing ===\")\n",
    "count = matcher.get_collection_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44105a28-58e0-43d2-b1bd-d995eda87f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://www.motortrend.com/news/honda-pricing-cr-2008/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53e13543-325f-4a7e-aac8-dcc825286a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL doesn't exist in database, here closer results\n",
      "\n",
      "1. Path: https://www.motortrend.com/news/2008-honda-s2000-cr-pricing/\n",
      "   Similarity: 1.0000\n",
      "2. Path: https://www.motortrend.com/news/2022-mazda-cx-30-pricing/\n",
      "   Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Find similar URLs\n",
    "closer_results = matcher.find_similar_url_by_path(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0807b-ec67-409c-ac4a-ac7052bbe307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
